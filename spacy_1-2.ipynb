{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Spacy class session for first week. \n",
    "\n",
    "Identify parts of speach and name entity\n",
    "How to use these extractions for downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# load english spacy model\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# prepare doc container\n",
    "\n",
    "with open('wiki_us.txt', 'r') as f:  # can also write (../ithaka/wiki_us_txt')  what is the difference\n",
    "    us_text = f.read()\n",
    "\n",
    "doc = nlp(us_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Boundary Detection - identification of sentences in a text. Better than using split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x1f60be99990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sents # view the generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access sentences in doc container\n",
    "\n",
    "for sent in doc.sents: # if adding () get a generator not callable error\n",
    "    print(sent)\n",
    "    print() # add whitespace between sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators only store a select amount of data as necessary, allows work on very large datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence1 \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49msents[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sentence1)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sentence1 = doc.sents[0]  # generates an error -- generator object is not subscriptable, cant be indexed\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "# render generator as a list to subscript, now every sentence is stored in memory as an object.\n",
    "\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 # looks like a string but is an object. To access it we need the .text attribute. Same is true for doc container\n",
    "\n",
    "type(sentence1)  # spacy.tokens.span.Span class, this this the type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1.text #allows us to access raw text\n",
    "\n",
    "type(sentence1.text) # now we are looking at a str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# we can do the same thing with a doc\n",
    "\n",
    "print(type(doc))\n",
    "print(type(doc.text))\n",
    "#doc.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Attributes - tokens contain a number of attributes (not functioning as raw strings) necessary to perform NLP. Assigned \n",
    "\n",
    ".text\n",
    ".head\n",
    ".left_edge\n",
    ".right_edge\n",
    ".ent_type-\n",
    ".iob_\n",
    ".lemma_\n",
    ".morph\n",
    ".pos_\n",
    ".dep_\n",
    ".lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States\n"
     ]
    }
   ],
   "source": [
    "# access token\n",
    "\n",
    "token2 = sentence1[2]\n",
    "print(token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'States'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access as raw text str\n",
    "\n",
    "token2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "The\n",
      "America\n",
      "384\n",
      "GPE\n"
     ]
    }
   ],
   "source": [
    "# Head - access syntactic 'parent'ArithmeticError\n",
    "\n",
    "print(token2.head) # tell us which word token2 is governed by\n",
    "\n",
    "# Left Edge\n",
    "\n",
    "print(token2.left_edge) #tells us where multi-word token begins, e.g., 'the united states' - multiple tokens that represent one idea\n",
    "\n",
    "# Right Edge - rightmost token of this tokens syntactic descendants\n",
    "\n",
    "print(token2.right_edge) \n",
    "\n",
    "# get entity type\n",
    "\n",
    "print(token2.ent_type) # returns an integer that represents entity label, _ will give the string equivalent\n",
    "\n",
    "print(token2.ent_type_) #GPE - geopolitical entities like a country\n",
    "\n",
    "print(token2.ent_iob_) #generate IOB -- not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets iterate over noun chunks - base noun phrase that has a noun as its head\n",
    "\n",
    "# create generator\n",
    "\n",
    "doc.noun_chunks\n",
    "\n",
    "# print off noun chunks\n",
    "\n",
    "chunks = list(doc.noun_chunks) # convert to list and iterate over\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known\n",
      "know\n"
     ]
    }
   ],
   "source": [
    "# Lemma - base form of the token, without inflectional suffixes\n",
    "\n",
    "token2.lemma_ # provides lemmetized form, 'states' - doesnt have a lemma, dealing with a proper noun ('united states) in this case\n",
    "\n",
    "print(sentence1[12]) #known\n",
    "\n",
    "print(sentence1[12].lemma_) #know\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Aspect=Perf|Tense=Past|VerbForm=Part"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MORPH - morphological analysis - tense, verb form, etc.\n",
    "\n",
    "sentence1[12].morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Part of Speach\n",
    "\n",
    "token2.pos_ # returns 'PROPN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntactic Dependancy - how it functions in sentence\n",
    "\n",
    "token2.dep_ # nsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Language - language of the parent documents vocabulary\n",
    "\n",
    "token2.lang_  # 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#st of the time we would be iterating over the doc text and using the attributes to collect particular information for each token\n",
    "\n",
    "doc = nlp(us_text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://spacy.io/universe/\n",
    "\n",
    "Selection of packages, plugins, extensions etc. E.g., can get native Latin NLP model, LatinCy. GreCy  is actually a transformer model for ancient Greece.\n",
    "\n",
    "Lots of stuff on this page\n",
    "\n",
    "displaCy - creates great visualizations\n",
    "\n",
    "Note: each component of spaCY pipeline has associated set of attributes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ithaka-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
