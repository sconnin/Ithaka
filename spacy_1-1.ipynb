{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI had the same issue. For me, easily resolved by launching VSC from the conda window.\\n\\nSpecifically, open your cmd prompt (for me, Anaconda Prompt), activate the environment using 'conda activate [envname]'. \\nThen just run the command 'code'. This will launch VS Code with the activated environment and associated variables. \\nFrom there, the debug works as expected.\\n\\nMy Constellate pswd is nitle!\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "I had the same issue. For me, easily resolved by launching VSC from the conda window.\n",
    "\n",
    "Specifically, open your cmd prompt (for me, Anaconda Prompt), activate the environment using 'conda activate [envname]'. \n",
    "Then just run the command 'code'. This will launch VS Code with the activated environment and associated variables. \n",
    "From there, the debug works as expected.\n",
    "\n",
    "My Constellate pswd is nitle!\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Ithaka'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an enlish language model - small \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“To Thomas Jefferson from George Wythe, 9 March 1770,” Founders Online, National Archives, https://founders.archives.gov/documents/Jefferson/01-01-02-0027. [Original source: The Papers of Thomas Jefferson, vol. 1, 1760–1776, ed. Julian P. Boyd. Princeton: Princeton University Press, 1950, p. 38.]\n",
      "I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his. You will also receive two of Foulis’s catalogues. Mrs. Wythe will send you some garden peas.\n",
      "You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.\n"
     ]
    }
   ],
   "source": [
    "with open ('fo_jefferson.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his. You will also receive two of Foulis’s catalogues. Mrs. Wythe will send you some garden peas.\n",
      "You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.\n"
     ]
    }
   ],
   "source": [
    "text = data.splitlines()[1:] # creates a list\n",
    "text ='\\n'.join(text) # creates a str\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775â€“1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanishâ€“American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n",
      "3525\n",
      "652\n",
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "U\n",
      "n\n",
      "i\n",
      "t\n",
      "e\n",
      "d\n",
      "The\n",
      "United\n",
      "States\n",
      "of\n",
      "America\n",
      "(\n",
      "U.S.A.\n",
      "or\n",
      "USA\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('wiki_us.txt', 'r') as f:\n",
    "    us_text = f.read()\n",
    "\n",
    "#create a doc container\n",
    "\n",
    "doc = nlp(us_text) # tokenize the text, doc object contains the tokens, nlp object is our pipeline, we pass in text\n",
    "\n",
    "print(doc)\n",
    "\n",
    "# tokens vs characters -- will see differnt length\n",
    "\n",
    "print(len(us_text)) # character based discrimination -  3521\n",
    "print(len(doc)) # token based discrimination - 654\n",
    "\n",
    "for token in us_text[:10]: # first 10 characters\n",
    "    print(token)\n",
    "\n",
    "for token in doc[:10]: # first 10  tokens\n",
    "    print(token)\n",
    "\n",
    "#  periods not separated as tokens in U.S.A > function collectively as same thing. Punctuations generally treated as distinct token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Taliaferro\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for some of his. You will also receive \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    two\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " of \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Foulis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       "’s catalogues. Mrs. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wythe\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " will send you some garden peas.</br>You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways. Durate, et vosmet rebus servate secundis.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split approach is naive - white spaces, here we use ML model, nlp pipeline\n",
    "# better to think of text as a sequence of tokens rather than as words\n",
    "\n",
    "doc = nlp(text) # run model on text and save to doc object\n",
    "\n",
    "displacy.render(doc, style='ent') # render name recognition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I send you some nectarine and apricot graffs and grapevines, the best I had; and have directed your messenger to call upon Major Taliaferro for some of his.\n",
      "You will also receive two of Foulis’s catalogues.\n",
      "Mrs. Wythe will send you some garden peas.\n",
      "\n",
      "You bear your misfortune so becomingly, that, as I am convinced you will surmount the difficulties it has plunged you into, so I foresee you will hereafter reap advantages from it several ways.\n",
      "Durate, et vosmet rebus servate secundis.\n"
     ]
    }
   ],
   "source": [
    "# to access sentences in the doc container, we can use the attributes sents\n",
    "# print each token for each sentence, can do quatitative analysis at sentence level. Can graph how frequently words occur across time\n",
    "\n",
    "\n",
    "for sent in doc.sents: # sents a container for sentences with metadata\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to analyze at the sentence level. THe different methods vary in terms of speed/accuracy tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes() # dictionary with sequential order of components of pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parser']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for large amounts of text\n",
    "\n",
    "nlp.disable_pipes('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes() # note that .sents is now gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.enable_pipe('senter') #adds senter to pipeline. Less accurate version of parser. Identifies start of sentences - less accurate but faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.disable_pipe('senter')\n",
    "nlp.add_pipe('sentencizer') # third way to analyze sentences, exceptionally quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()\n",
    "\n",
    "# note word tokenization functions at token level. Sentence tokenization operates at sentence boundaries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ithaka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
